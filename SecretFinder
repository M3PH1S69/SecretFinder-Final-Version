#!/usr/bin/env python3

import os
import sys
import re
import glob
import argparse
import jsbeautifier
import webbrowser
import subprocess
import base64
import requests
import string
import random
from html import escape
import urllib3
import xml.etree.ElementTree as ET
from requests_file import FileAdapter
from lxml import html
from urllib.parse import urlparse
from typing import List, Dict, Union, Optional, Tuple, Any

PROGRAM_VERSION = "1.0.1"

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

def clear_screen():
    os.system('clear' if os.name == 'posix' else 'cls')

def show_banner():
    clear_screen()
    print(f"{Colors.PURPLE}{'#' * 46}{Colors.END}")
    print(f"{Colors.BOLD}       BugBounty-Automation BY M3PH1S69")
    print(f"{Colors.BOLD}          You want to try this tool? ")
    print(f"{Colors.BOLD}        Contact me: t.me/SeacrhSploit")
    print(f"{Colors.PURPLE}{'#' * 46}{Colors.END}\n")

def show_version():
    print(f"SecretFinder Version: {PROGRAM_VERSION}")
    sys.exit(0)

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
REQUEST_TIMEOUT = 30  # 30 seconds

# HTML template
HTML_TEMPLATE = """<!DOCTYPE html>
<html>
<head>
    <title>SecretFinder Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; font-size: 20px; }
        .text { background-color: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 3px; }
        .container { background-color: #fff; padding: 10px; margin: 5px 0 15px; border: 1px solid #ddd; border-radius: 3px; }
        span { background-color: yellow; }
    </style>
</head>
<body>
    $$content$$
</body>
</html>"""

# Regex patterns
REGEX_PATTERNS = {
    'google_api': r'AIza[0-9A-Za-z-_]{35}',
    'firebase': r'AAAA[A-Za-z0-9_-]{7}:[A-Za-z0-9_-]{140}',
    'google_captcha': r'6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$',
    'google_oauth': r'ya29\.[0-9A-Za-z\-_]+',
    'amazon_aws_access_key_id': r'A[SK]IA[0-9A-Z]{16}',
    'amazon_mws_auth_token': r'amzn\\.mws\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
    'amazon_aws_url': r's3\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\.s3\.amazonaws.com',
    'amazon_aws_url2': r"([a-zA-Z0-9-\.\_]+\.s3\.amazonaws\.com|s3://[a-zA-Z0-9-\.\_]+|s3-[a-zA-Z0-9-\.\_\/]+|s3.amazonaws.com/[a-zA-Z0-9-\.\_]+|s3.console.aws.amazon.com/s3/buckets/[a-zA-Z0-9-\.\_]+)",
    'facebook_access_token': r'EAACEdEose0cBA[0-9A-Za-z]+',
    'authorization_basic': r'basic [a-zA-Z0-9=:_\+\/-]{5,100}',
    'authorization_bearer': r'bearer [a-zA-Z0-9_\-\.=:_\+\/]{5,100}',
    'authorization_api': r'api[key|_key|\s+]+[a-zA-Z0-9_\-]{5,100}',
    'mailgun_api_key': r'key-[0-9a-zA-Z]{32}',
    'twilio_api_key': r'SK[0-9a-fA-F]{32}',
    'twilio_account_sid': r'AC[a-zA-Z0-9_\-]{32}',
    'twilio_app_sid': r'AP[a-zA-Z0-9_\-]{32}',
    'paypal_braintree_access_token': r'access_token\$production\$[0-9a-z]{16}\$[0-9a-f]{32}',
    'square_oauth_secret': r'sq0csp-[ 0-9A-Za-z\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\-_]{22,43}',
    'square_access_token': r'sqOatp-[0-9A-Za-z\-_]{22}|EAAA[a-zA-Z0-9]{60}',
    'stripe_standard_api': r'sk_live_[0-9a-zA-Z]{24}',
    'stripe_restricted_api': r'rk_live_[0-9a-zA-Z]{24}',
    'github_access_token': r'[a-zA-Z0-9_-]*:[a-zA-Z0-9_\-]+@github\.com*',
    'rsa_private_key': r'-----BEGIN RSA PRIVATE KEY-----',
    'ssh_dsa_private_key': r'-----BEGIN DSA PRIVATE KEY-----',
    'ssh_dc_private_key': r'-----BEGIN EC PRIVATE KEY-----',
    'pgp_private_block': r'-----BEGIN PGP PRIVATE KEY BLOCK-----',
    'json_web_token': r'ey[A-Za-z0-9-_=]+\.[A-Za-z0-9-_=]+\.?[A-Za-z0-9-_.+/=]*$',
    'slack_token': r"\"api_token\":\"(xox[a-zA-Z]-[a-zA-Z0-9-]+)\"",
    'SSH_privKey': r"([-]+BEGIN [^\s]+ PRIVATE KEY[-]+[\s]*[^-]*[-]+END [^\s]+ PRIVATE KEY[-]+)",
    'Heroku API KEY': r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}',
    'possible_Creds': r"(?i)(password\s*[`=:\"]+\s*[^\s]+|password is\s*[`=:\"]*\s*[^\s]+|pwd\s*[`=:\"]*\s*[^\s]+|passwd\s*[`=:\"]+\s*[^\s]+)",
}

def parser_error(msg: str) -> None:
    """Print error message and exit with error code."""
    print(f'Usage: python {sys.argv[0]} [OPTIONS] use -h for help')
    print(f'Error: {msg}')
    sys.exit(1)

def get_context(matches: List[Tuple[str, int, int]], content: str, name: str, rex: str = '.+?') -> List[Dict[str, Any]]:
    """Get context around matches."""
    items = []
    unique_matches = list({x[0] for x in matches})  # Get unique matches
    
    for match in unique_matches:
        try:
            context_regex = re.compile(f'{rex}{re.escape(match)}{rex}', re.IGNORECASE)
            contexts = context_regex.findall(content)
            
            item = {
                'matched': match,
                'name': name,
                'context': contexts,
                'multi_context': len(contexts) > 1
            }
            items.append(item)
        except re.error as e:
            print(f"Error creating context regex for {match}: {e}")
            continue
    
    return items

def parser_file(content: str, mode: int = 1, more_regex: Optional[str] = None, no_dup: bool = True) -> List[Dict[str, Any]]:
    """Parse file content for secrets with binary file detection."""
    # Binary file detection
    if isinstance(content, bytes) or (isinstance(content, str) and '\0' in content[:1024]):
        print("[!] Warning: Binary file detected, skipping beautification")
        mode = 0
    
    if mode == 1 and len(content) > 1000000:
        content = content.replace(";", ";\r\n").replace(",", ",\r\n")
    elif mode == 1:
        try:
            content = jsbeautifier.beautify(content)
        except Exception as e:
            print(f"Error beautifying content: {e}")
    
    all_items = []
    
    for name, pattern in REGEX_PATTERNS.items():
        try:
            compiled_pattern = re.compile(pattern, re.VERBOSE | re.IGNORECASE)
            matches = [(m.group(0), m.start(0), m.end(0)) for m in compiled_pattern.finditer(content)]
            
            if mode == 1:
                items = get_context(matches, content, name)
            else:
                items = [{
                    'matched': match[0],
                    'context': [],
                    'name': name,
                    'multi_context': False
                } for match in matches]
            
            if items:
                all_items.extend(items)
        except Exception as e:
            print(f"Error processing regex {name}: {e}")
            continue

    # Remove duplicates if requested
    if no_dup:
        seen = set()
        all_items = [item for item in all_items 
                    if item['matched'] not in seen and not seen.add(item['matched'])]

    # Apply additional regex filter if provided
    if more_regex:
        try:
            extra_filter = re.compile(more_regex)
            all_items = [item for item in all_items if extra_filter.search(item['matched'])]
        except Exception as e:
            print(f"Invalid additional regex: {e}")

    return all_items

def parser_input(input_str: str) -> Union[List[str], List[Dict[str, str]]]:
    """Parse input which can be URL, file, or directory."""
    schemes = ('http://', 'https://', 'ftp://', 'file://', 'ftps://')
    
    # URL case
    if input_str.startswith(schemes):
        return [input_str]
    
    # View-source case
    if input_str.startswith('view-source:'):
        return [input_str[12:]]
    
    # Burp file case
    if hasattr(args, 'burp') and args.burp:
        try:
            with open(args.input, 'r', encoding='utf-8') as f:
                items = ET.fromstring(f.read())
                
            return [{
                'js': base64.b64decode(item.find('response').text).decode('utf-8', 'replace'),
                'url': item.find('url').text
            } for item in items]
        except Exception as err:
            parser_error(f"Failed to parse Burp file: {err}")
    
    # Wildcard case
    if '*' in input_str:
        paths = glob.glob(os.path.abspath(input_str))
        if not paths:
            parser_error('Input with wildcard does not match any files.')
        return [f"file://{path}" for path in paths]
    
    # Local file case
    path = os.path.abspath(input_str)
    if not os.path.exists(path):
        parser_error('File could not be found (maybe you forgot to add http/https).')
    return [f"file://{path}"]

def html_save(output: str) -> None:
    """Save results to HTML file."""
    try:
        output_path = os.path.abspath(args.output)
        with open(output_path, "w", encoding='utf-8') as text_file:
            html_content = HTML_TEMPLATE.replace('$$content$$', output)
            text_file.write(html_content)
        
        print(f'URL to access output: file://{output_path}')
        file_url = f'file://{output_path}'
        
        if sys.platform.startswith('linux'):
            subprocess.call(['xdg-open', file_url])
        else:
            webbrowser.open(file_url)
    except Exception as err:
        print(f"Output can't be saved in {args.output} due to exception: {err}")

def cli_output(matched: List[Dict[str, Any]]) -> None:
    """Print results to command line with improved formatting."""
    if not matched:
        print("No secrets found.")
        return
    
    max_name_len = max(len(m.get('name', '')) for m in matched) + 2
    print("\nFound secrets:")
    print("-" * (max_name_len + 50))
    
    for match in matched:
        try:
            name = match.get('name', 'unknown').replace('_', ' ').title()
            value = match.get('matched', '')
            
            # Truncate value if too long
            if len(value) > 50:
                value = value[:47] + "..."
            
            print(f"{name.ljust(max_name_len)} -> {value}")
            
            # Show context if available
            if match.get('context'):
                print(" " * max_name_len + "   Context:")
                for ctx in set(match['context']):  # Remove duplicate contexts
                    print(" " * max_name_len + f"   - {ctx[:100]}{'...' if len(ctx) > 100 else ''}")
        except UnicodeEncodeError:
            print(f"{match.get('name', 'unknown').ljust(max_name_len)} -> [Contains non-ASCII characters]")
    
    print("-" * (max_name_len + 50))
    print(f"Total secrets found: {len(matched)}")

def extract_js_urls(content: str, base_url: str) -> List[str]:
    """Extract JavaScript URLs from HTML content with improved error handling."""
    try:
        # Skip if content is empty or not HTML
        if not content or ('<html' not in content.lower() and '<script' not in content.lower()):
            return []
            
        soup = html.fromstring(content)
        all_src = []
        
        parsed_url = urlparse(base_url)
        base_root = f"{parsed_url.scheme}://{parsed_url.netloc}"
        base_path = f"{base_root}/{parsed_url.path.rsplit('/', 1)[0]}" if '/' in parsed_url.path else base_root
        
        for src in soup.xpath('//script/@src'):
            if not src:
                continue
                
            try:
                if src.startswith(('http://', 'https://', 'ftp://', 'ftps://')):
                    all_src.append(src)
                elif src.startswith('//'):
                    all_src.append(f"{parsed_url.scheme}:{src}")
                elif src.startswith('/'):
                    all_src.append(f"{base_root}{src}")
                else:
                    all_src.append(f"{base_path}/{src.lstrip('/')}")
            except Exception as e:
                print(f"Error processing URL {src}: {e}")
                continue
        
        # Apply filters
        if hasattr(args, 'ignore') and args.ignore:
            ignore_list = args.ignore.split(';')
            all_src = [src for src in all_src if not any(i.lower() in src.lower() for i in ignore_list)]
            
        if hasattr(args, 'only') and args.only:
            only_list = args.only.split(';')
            all_src = [src for src in all_src if any(o.lower() in src.lower() for o in only_list)]
            
        return list(set(all_src))  # Remove duplicates
        
    except Exception as e:
        print(f"Error extracting JS URLs: {e}")
        return []

def send_request(url: str) -> str:
    """Send HTTP request and return content with improved security."""
    # Validate URL to prevent SSRF
    parsed = urlparse(url)
    if parsed.scheme not in ('http', 'https', 'file', 'ftp', 'ftps'):
        parser_error(f"Unsupported URL scheme: {parsed.scheme}")
    
    # Local file case
    if url.startswith('file://'):
        try:
            s = requests.Session()
            s.mount('file://', FileAdapter())
            response = s.get(url, timeout=REQUEST_TIMEOUT)
            return response.content.decode('utf-8', 'replace')
        except Exception as e:
            parser_error(f"Failed to read local file: {e}")
    
    # Prepare headers
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Accept': 'text/html, application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.8',
        'Accept-Encoding': 'gzip'
    }
    
    # Add custom headers
    if hasattr(args, 'headers') and args.headers:
        try:
            for header in args.headers.split('\\n'):
                if ':' in header:
                    name, value = header.split(':', 1)
                    headers[name.strip()] = value.strip()
        except Exception as e:
            print(f"Error parsing headers: {e}")
    
    # Add cookies if provided
    if hasattr(args, 'cookie') and args.cookie:
        headers['Cookie'] = args.cookie
    
    # Set up proxies
    proxies = {}
    if hasattr(args, 'proxy') and args.proxy:
        proxies = {
            'http': args.proxy,
            'https': args.proxy
        }
    
    try:
        # Make SSL verification optional
        verify_ssl = not args.insecure if hasattr(args, 'insecure') else True
        
        response = requests.get(
            url=url,
            verify=verify_ssl,
            headers=headers,
            proxies=proxies,
            timeout=REQUEST_TIMEOUT,
            stream=True
        )
        
        # Check file size before downloading
        content_length = response.headers.get('Content-Length')
        if content_length and int(content_length) > MAX_FILE_SIZE:
            parser_error(f"File too large (>{MAX_FILE_SIZE/1024/1024}MB)")
        
        # Use iter_content for large files
        content = []
        for chunk in response.iter_content(chunk_size=8192, decode_unicode=True):
            if chunk:
                content.append(chunk)
            if len(b''.join(content)) > MAX_FILE_SIZE:
                parser_error(f"File too large during streaming (>{MAX_FILE_SIZE/1024/1024}MB)")
        
        return ''.join(content)
    except requests.exceptions.RequestException as err:
        parser_error(f"Request failed: {err}")

if __name__ == "__main__":
    show_banner()
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--extract", help="Extract all JavaScript links located in a page and process it", action="store_true")
    parser.add_argument("-i", "--input", help="Input a: URL, file or folder", required=True)
    parser.add_argument("-o", "--output", help="Where to save the file, including file name. Default: output.html", default="output.html")
    parser.add_argument("-r", "--regex", help="RegEx for filtering purposes against found endpoint")
    parser.add_argument("-b", "--burp", help="Support burp exported file", action="store_true")
    parser.add_argument("-c", "--cookie", help="Add cookies for authenticated JS files", default="")
    parser.add_argument("-g", "--ignore", help="Ignore js url if it contains the provided string (string;string2..)", default="")
    parser.add_argument("-n", "--only", help="Process js url only if it contains the provided string (string;string2..)", default="")
    parser.add_argument("-H", "--headers", help="Set headers (\"Name:Value\\nName:Value\")", default="")
    parser.add_argument("-p", "--proxy", help="Set proxy (host:port)", default="")
    parser.add_argument("-k", "--insecure", help="Allow insecure SSL connections", action="store_true")
    parser.add_argument("-v", "--version", action="store_true", help="Show program version")
    
    args = parser.parse_args()
    
    # Normalize input path
    args.input = args.input.rstrip("/")
    
    # Validate regex if provided
    if args.version:
        show_version()

    if args.regex:
        try:
            test_str = ''.join(random.choices(string.ascii_letters + string.digits, k=50))
            re.compile(args.regex)
            REGEX_PATTERNS['custom_regex'] = args.regex
        except re.error as e:
            parser_error(f"Invalid regular expression: {e}")
    
    # Process input
    try:
        if args.extract:
            content = send_request(args.input)
            urls = extract_js_urls(content, args.input)
        else:
            urls = parser_input(args.input)
        
        # Process URLs
        output = ''
        for url in urls:
            print(f"[ + ] Processing: {url}")
            
            if not hasattr(args, 'burp') or not args.burp:
                content = send_request(url)
                current_url = url
            else:
                content = url.get('js')
                current_url = url.get('url')
            
            matched = parser_file(content, mode=0 if args.output == "cli" else 1)
            
            if args.output == 'cli':
                cli_output(matched)
            else:
                output += f'<h1>File: <a href="{escape(current_url)}" target="_blank">{escape(current_url)}</a></h1>'
                
                for match in matched:
                    header = f'<div class="text">{match["name"].replace("_", " ")}'
                    body = ''
                    
                    if match['multi_context']:
                        seen_contexts = set()
                        for context in match['context']:
                            if context not in seen_contexts:
                                highlighted = context.replace(
                                    match['matched'], 
                                    f'<span style="background-color:yellow">{match["matched"]}</span>')
                                body += f'</a><div class="container">{highlighted}</div></div>'
                                seen_contexts.add(context)
                    else:
                        context = match['context'][0] if match['context'] else ''
                        if context:
                            highlighted = context.replace(
                                match['matched'],
                                f'<span style="background-color:yellow">{match["matched"]}</span>')
                            body += f'</a><div class="container">{highlighted}</div></div>'
                    
                    output += header + body
        
        if args.output != 'cli' and output:
            html_save(output)
    except KeyboardInterrupt:
        print("\n[!] Interrupted by user")
        sys.exit(1)
    except Exception as e:
        parser_error(f"Unexpected error: {e}")
