#!/usr/bin/env python3

import os
import sys
import re
import glob
import argparse
import jsbeautifier
import webbrowser
import subprocess
import base64
import requests
import string
import random
from html import escape
import urllib3
import xml.etree.ElementTree
from requests_file import FileAdapter
from lxml import html
from urllib.parse import urlparse
from typing import List, Dict, Union, Optional

#######################################################################
######              BUG_BOUNTY_AUTOMATION_v1 BY M3PH1S           ######
######               NOTE: You want to try this tool?            ######
######                Contact me: t.me/SeacrhSploit              ######
#######################################################################

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
REQUEST_TIMEOUT = 30  # 30 seconds

# HTML template
_template = """<!DOCTYPE html>
<html>
<head>
    <title>SecretFinder Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; font-size: 20px; }
        .text { background-color: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 3px; }
        .container { background-color: #fff; padding: 10px; margin: 5px 0 15px; border: 1px solid #ddd; border-radius: 3px; }
        span { background-color: yellow; }
    </style>
</head>
<body>
    $$content$$
</body>
</html>"""

# Regex patterns
_regex = {
    'google_api': r'AIza[0-9A-Za-z-_]{35}',
    'firebase': r'AAAA[A-Za-z0-9_-]{7}:[A-Za-z0-9_-]{140}',
    'google_captcha': r'6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$',
    'google_oauth': r'ya29\.[0-9A-Za-z\-_]+',
    'amazon_aws_access_key_id': r'A[SK]IA[0-9A-Z]{16}',
    'amazon_mws_auth_token': r'amzn\\.mws\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
    'amazon_aws_url': r's3\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\.s3\.amazonaws.com',
    'amazon_aws_url2': r"([a-zA-Z0-9-\.\_]+\.s3\.amazonaws\.com|s3://[a-zA-Z0-9-\.\_]+|s3-[a-zA-Z0-9-\.\_\/]+|s3.amazonaws.com/[a-zA-Z0-9-\.\_]+|s3.console.aws.amazon.com/s3/buckets/[a-zA-Z0-9-\.\_]+)",
    'facebook_access_token': r'EAACEdEose0cBA[0-9A-Za-z]+',
    'authorization_basic': r'basic [a-zA-Z0-9=:_\+\/-]{5,100}',
    'authorization_bearer': r'bearer [a-zA-Z0-9_\-\.=:_\+\/]{5,100}',
    'authorization_api': r'api[key|_key|\s+]+[a-zA-Z0-9_\-]{5,100}',
    'mailgun_api_key': r'key-[0-9a-zA-Z]{32}',
    'twilio_api_key': r'SK[0-9a-fA-F]{32}',
    'twilio_account_sid': r'AC[a-zA-Z0-9_\-]{32}',
    'twilio_app_sid': r'AP[a-zA-Z0-9_\-]{32}',
    'paypal_braintree_access_token': r'access_token\$production\$[0-9a-z]{16}\$[0-9a-f]{32}',
    'square_oauth_secret': r'sq0csp-[ 0-9A-Za-z\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\-_]{22,43}',
    'square_access_token': r'sqOatp-[0-9A-Za-z\-_]{22}|EAAA[a-zA-Z0-9]{60}',
    'stripe_standard_api': r'sk_live_[0-9a-zA-Z]{24}',
    'stripe_restricted_api': r'rk_live_[0-9a-zA-Z]{24}',
    'github_access_token': r'[a-zA-Z0-9_-]*:[a-zA-Z0-9_\-]+@github\.com*',
    'rsa_private_key': r'-----BEGIN RSA PRIVATE KEY-----',
    'ssh_dsa_private_key': r'-----BEGIN DSA PRIVATE KEY-----',
    'ssh_dc_private_key': r'-----BEGIN EC PRIVATE KEY-----',
    'pgp_private_block': r'-----BEGIN PGP PRIVATE KEY BLOCK-----',
    'json_web_token': r'ey[A-Za-z0-9-_=]+\.[A-Za-z0-9-_=]+\.?[A-Za-z0-9-_.+/=]*$',
    'slack_token': r"\"api_token\":\"(xox[a-zA-Z]-[a-zA-Z0-9-]+)\"",
    'SSH_privKey': r"([-]+BEGIN [^\s]+ PRIVATE KEY[-]+[\s]*[^-]*[-]+END [^\s]+ PRIVATE KEY[-]+)",
    'Heroku API KEY': r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}',
    'possible_Creds': r"(?i)(password\s*[`=:\"]+\s*[^\s]+|password is\s*[`=:\"]*\s*[^\s]+|pwd\s*[`=:\"]*\s*[^\s]+|passwd\s*[`=:\"]+\s*[^\s]+)",
}

def parser_error(msg: str) -> None:
    """Print error message and exit with error code"""
    print(f'Usage: python {sys.argv[0]} [OPTIONS] use -h for help')
    print(f'Error: {msg}')
    sys.exit(1)

def getContext(matches: List[tuple], content: str, name: str, rex: str = '.+?') -> List[Dict]:
    """Get context around matches"""
    items = []
    unique_matches = list({x[0] for x in matches})  # Get unique matches
    
    # Pre-compile regex for performance
    context_regex = re.compile(f'{rex}{re.escape(match)}{rex}', re.IGNORECASE)
    
    for match in unique_matches:
        contexts = context_regex.findall(content)
        
        item = {
            'matched': match,
            'name': name,
            'context': contexts,
            'multi_context': len(contexts) > 1
        }
        items.append(item)
    
    return items

def parser_file(content: str, mode: int = 1, more_regex: Optional[str] = None, no_dup: int = 1) -> List[Dict]:
    """Parse file content for secrets with binary file detection"""
    # Deteksi binary file
    if isinstance(content, bytes) or '\0' in content:
        print("[!] Warning: Binary file detected, skipping beautification")
        mode = 0
    
    if mode == 1 and len(content) > 1000000:
        content = content.replace(";", ";\r\n").replace(",", ",\r\n")
    elif mode == 1:
        try:
            content = jsbeautifier.beautify(content)
        except Exception as e:
            print(f"Error beautifying content: {e}")
    
    all_items = []
    
    # Kompilasi semua regex terlebih dahulu untuk performa
    compiled_regex = {}
    for name, pattern in _regex.items():
        try:
            compiled_regex[name] = re.compile(pattern, re.VERBOSE | re.I)
        except re.error as e:
            print(f"Error compiling regex {name}: {e}")
            continue
    
    for name, pattern in compiled_regex.items():
        try:
            matches = [(m.group(0), m.start(0), m.end(0)) for m in re.finditer(pattern, content)]
            
            if mode == 1:
                items = getContext(matches, content, name)
            else:
                items = [{
                    'matched': m.group(0),
                    'context': [],
                    'name': name,
                    'multi_context': False
                } for m in re.finditer(pattern, content)]
            
            if items:
                all_items.extend(items)
                
        except Exception as e:
            print(f"Error processing regex {name}: {e}")
            continue

    # Remove duplicates if requested
    if no_dup:
        seen = set()
        all_items = [item for item in all_items 
                    if item['matched'] not in seen and not seen.add(item['matched'])]

    # Apply additional regex filter if provided
    if more_regex:
        try:
            extra_filter = re.compile(more_regex)
            all_items = [item for item in all_items if extra_filter.search(item['matched'])]
        except Exception as e:
            print(f"Invalid additional regex: {e}")

    return all_items

def parser_input(input_str: str) -> Union[List[str], List[Dict]]:
    """Parse input which can be URL, file, or directory"""
    schemes = ('http://', 'https://', 'ftp://', 'file://', 'ftps://')
    
    # URL case
    if input_str.startswith(schemes):
        return [input_str]
    
    # View-source case
    if input_str.startswith('view-source:'):
        return [input_str[12:]]
    
    # Burp file case
    if args.burp:
        try:
            with open(args.input, 'r') as f:
                items = xml.etree.ElementTree.fromstring(f.read())
                
            return [{
                'js': base64.b64decode(item.find('response').text).decode('utf-8', 'replace'),
                'url': item.find('url').text
            } for item in items]
            
        except Exception as err:
            parser_error(f"Failed to parse Burp file: {err}")
    
    # Wildcard case
    if '*' in input_str:
        paths = glob.glob(os.path.abspath(input_str))
        if not paths:
            parser_error('Input with wildcard does not match any files.')
        return [f"file://{path}" for path in paths]
    
    # Local file case
    path = os.path.abspath(input_str)
    if not os.path.exists(path):
        parser_error('File could not be found (maybe you forgot to add http/https).')
    return [f"file://{path}"]

def html_save(output: str) -> None:
    """Save results to HTML file"""
    try:
        with open(args.output, "wb") as text_file:
            html_content = _template.replace('$$content$$', output)
            text_file.write(html_content.encode('utf-8'))
        
        print(f'URL to access output: file://{os.path.abspath(args.output)}')
        file_url = f'file://{os.path.abspath(args.output)}'
        
        if sys.platform.startswith('linux'):
            subprocess.call(['xdg-open', file_url])
        else:
            webbrowser.open(file_url)
            
    except Exception as err:
        print(f"Output can't be saved in {args.output} due to exception: {err}")

def cli_output(matched: List[Dict]) -> None:
    """Print results to command line with improved formatting"""
    if not matched:
        print("No secrets found.")
        return
    
    max_name_len = max(len(m.get('name', '')) for m in matched) + 2
    print("\nFound secrets:")
    print("-" * (max_name_len + 50))
    
    for match in matched:
        try:
            name = match.get('name', 'unknown').replace('_', ' ').title()
            value = match.get('matched', '')
            
            # Potong value jika terlalu panjang
            if len(value) > 50:
                value = value[:47] + "..."
            print(f"{name.ljust(max_name_len)} -> {value}")
            
            # Tampilkan context jika ada
            if match.get('context'):
                print(" " * max_name_len + "   Context:")
                for ctx in set(match['context']):  # Hapus duplikat context
                    print(" " * max_name_len + f"   - {ctx[:100]}{'...' if len(ctx) > 100 else ''}")
            
        except UnicodeEncodeError:
            print(f"{match.get('name', 'unknown').ljust(max_name_len)} -> [Contains non-ASCII characters]")
    
    print("-" * (max_name_len + 50))
    print(f"Total secrets found: {len(matched)}")

def extractjsurl(content: str, base_url: str) -> List[str]:
    """Extract JavaScript URLs from HTML content with improved error handling"""
    try:
        # Skip jika content kosong atau bukan HTML
        if not content or '<html' not in content.lower():
            return []
            
        soup = html.fromstring(content)
        all_src = []
        
        parsed_url = urlparse(base_url)
        base_root = f"{parsed_url.scheme}://{parsed_url.netloc}"
        base_path = f"{base_root}/{parsed_url.path.rsplit('/', 1)[0]}" if '/' in parsed_url.path else base_root
        
        for src in soup.xpath('//script/@src'):
            if not src:
                continue
                
            try:
                if src.startswith(('http://', 'https://', 'ftp://', 'ftps://')):
                    all_src.append(src)
                elif src.startswith('//'):
                    all_src.append(f"{parsed_url.scheme}:{src}")
                elif src.startswith('/'):
                    all_src.append(f"{base_root}{src}")
                else:
                    all_src.append(f"{base_path}/{src}")
            except Exception as e:
                print(f"Error processing URL {src}: {e}")
                continue
        
        # Apply filters
        if args.ignore:
            ignore_list = args.ignore.split(';')
            all_src = [src for src in all_src if not any(i.lower() in src.lower() for i in ignore_list)]
            
        if args.only:
            only_list = args.only.split(';')
            all_src = [src for src in all_src if any(o.lower() in src.lower() for o in only_list)]
            
        return list(set(all_src))  # Remove duplicates
        
    except Exception as e:
        print(f"Error extracting JS URLs: {e}")
        return []

def send_request(url: str) -> str:
    """Send HTTP request and return content with improved security"""
    # Validasi URL untuk mencegah SSRF
    parsed = urlparse(url)
    if parsed.scheme not in ('http', 'https', 'file', 'ftp', 'ftps'):
        parser_error(f"Unsupported URL scheme: {parsed.scheme}")
    
    # Local file case
    if url.startswith('file://'):
        try:
            s = requests.Session()
            s.mount('file://', FileAdapter())
            response = s.get(url, timeout=REQUEST_TIMEOUT)
            return response.content.decode('utf-8', 'replace')
        except Exception as e:
            parser_error(f"Failed to read local file: {e}")
    
    # Prepare headers
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Accept': 'text/html, application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.8',
        'Accept-Encoding': 'gzip'
    }
    
    # Add custom headers
    if args.headers:
        try:
            for header in args.headers.split('\\n'):
                if ':' in header:
                    name, value = header.split(':', 1)
                    headers[name.strip()] = value.strip()
        except Exception as e:
            print(f"Error parsing headers: {e}")
    
    # Add cookies if provided
    if args.cookie:
        headers['Cookie'] = args.cookie
    
    # Set up proxies
    proxies = {}
    if args.proxy:
        proxies = {
            'http': args.proxy,
            'https': args.proxy
        }
    
    try:
        # Buat verifikasi SSL menjadi opsional
        verify_ssl = not args.insecure if hasattr(args, 'insecure') else True
        
        response = requests.get(
            url=url,
            verify=verify_ssl,
            headers=headers,
            proxies=proxies,
            timeout=REQUEST_TIMEOUT,
            stream=True
        )
        
        # Check file size before downloading
        content_length = response.headers.get('Content-Length')
        if content_length and int(content_length) > MAX_FILE_SIZE:
            parser_error(f"File too large (>{MAX_FILE_SIZE/1024/1024}MB)")
        
        # Gunakan iter_content untuk file besar
        content = []
        for chunk in response.iter_content(chunk_size=8192, decode_unicode=True):
            if chunk:
                content.append(chunk)
            if len(b''.join(content)) > MAX_FILE_SIZE:
                parser_error(f"File too large during streaming (>{MAX_FILE_SIZE/1024/1024}MB)")
        
        return ''.join(content)
        
    except requests.exceptions.RequestException as err:
        parser_error(f"Request failed: {err}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--extract", help="Extract all JavaScript links located in a page and process it", action="store_true")
    parser.add_argument("-i", "--input", help="Input a: URL, file or folder", required=True)
    parser.add_argument("-o", "--output", help="Where to save the file, including file name. Default: output.html", default="output.html")
    parser.add_argument("-r", "--regex", help="RegEx for filtering purposes against found endpoint")
    parser.add_argument("-b", "--burp", help="Support burp exported file", action="store_true")
    parser.add_argument("-c", "--cookie", help="Add cookies for authenticated JS files", default="")
    parser.add_argument("-g", "--ignore", help="Ignore js url if it contains the provided string (string;string2..)", default="")
    parser.add_argument("-n", "--only", help="Process js url only if it contains the provided string (string;string2..)", default="")
    parser.add_argument("-H", "--headers", help="Set headers (\"Name:Value\\nName:Value\")", default="")
    parser.add_argument("-p", "--proxy", help="Set proxy (host:port)", default="")
    parser.add_argument("-k", "--insecure", help="Allow insecure SSL connections", action="store_true")
    
    args = parser.parse_args()
    
    # Normalize input path
    args.input = args.input.rstrip("/")
    
    # Validate regex if provided
    if args.regex:
        try:
            test_str = ''.join(random.choices(string.ascii_letters + string.digits, k=50))
            re.compile(args.regex)
            _regex['custom_regex'] = args.regex
        except re.error as e:
            parser_error(f"Invalid regular expression: {e}")
    
    # Process input
    try:
        if args.extract:
            content = send_request(args.input)
            urls = extractjsurl(content, args.input)
        else:
            urls = parser_input(args.input)
        
        # Process URLs
        output = ''
        for url in urls:
            print(f"[ + ] Processing: {url}")
            
            if not args.burp:
                content = send_request(url)
                current_url = url
            else:
                content = url.get('js')
                current_url = url.get('url')
            
            matched = parser_file(content, mode=0 if args.output == "cli" else 1)
            
            if args.output == 'cli':
                cli_output(matched)
            else:
                output += f'<h1>File: <a href="{escape(current_url)}" target="_blank">{escape(current_url)}</a></h1>'
                
                for match in matched:
                    header = f'<div class="text">{match["name"].replace("_", " ")}'
                    body = ''
                    
                    if match['multi_context']:
                        seen_contexts = set()
                        for context in match['context']:
                            if context not in seen_contexts:
                                highlighted = context.replace(
                                    match['matched'], 
                                    f'<span style="background-color:yellow">{match["matched"]}</span>')
                                body += f'</a><div class="container">{highlighted}</div></div>'
                                seen_contexts.add(context)
                    else:
                        context = match['context'][0] if match['context'] else ''
                        if context:
                            highlighted = context.replace(
                                match['matched'],
                                f'<span style="background-color:yellow">{match["matched"]}</span>')
                            body += f'</a><div class="container">{highlighted}</div></div>'
                    
                    output += header + body
        
        if args.output != 'cli' and output:
            html_save(output)
            
    except KeyboardInterrupt:
        print("\n[!] Interrupted by user")
        sys.exit(1)
    except Exception as e:
        parser_error(f"Unexpected error: {e}")
